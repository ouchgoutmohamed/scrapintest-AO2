"""
Test rapide d'extraction avec v√©rification des URLs d√©taill√©es
Ce script teste le spider sans sauvegarder en base de donn√©es
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Cr√©er un SelectorEventLoop pour Windows (Playwright n√©cessite asyncio)
import asyncio
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
else:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

from twisted.internet import asyncioreactor
asyncioreactor.install(loop)

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy import signals
from pprint import pprint
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s'
)

# Statistiques collect√©es
stats_collector = {
    'items_with_url': 0,
    'items_without_url': 0,
    'total_items': 0,
    'sample_items': []
}


class ItemCollector:
    """Collecteur d'items pour analyse"""
    
    def __init__(self):
        self.items = []
    
    def item_scraped(self, item, response, spider):
        """Appel√© quand un item est extrait"""
        self.items.append(dict(item))
        
        # V√©rifier la pr√©sence de l'URL
        url_detail = item.get('url_detail', '')
        
        stats_collector['total_items'] += 1
        
        if url_detail and url_detail.strip():
            stats_collector['items_with_url'] += 1
            print(f"\n‚úì Item extrait avec URL:")
            print(f"  Ref: {item.get('ref_consultation', 'N/A')}")
            print(f"  Titre: {item.get('titre', 'N/A')[:60]}...")
            print(f"  URL: {url_detail}")
        else:
            stats_collector['items_without_url'] += 1
            print(f"\n‚úó Item SANS URL:")
            print(f"  Ref: {item.get('ref_consultation', 'N/A')}")
            print(f"  Titre: {item.get('titre', 'N/A')[:60]}...")
        
        # Garder quelques exemples
        if len(stats_collector['sample_items']) < 5:
            stats_collector['sample_items'].append({
                'ref': item.get('ref_consultation'),
                'titre': item.get('titre', '')[:80],
                'organisme': item.get('organisme_acronyme'),
                'url_detail': url_detail,
                'has_url': bool(url_detail and url_detail.strip())
            })


def run_test():
    """Lance le test d'extraction"""
    
    print("\n" + "="*70)
    print("  TEST D'EXTRACTION - V√âRIFICATION DES URLs D√âTAILL√âES")
    print("="*70)
    print("\nCe test va:")
    print("  1. Extraire quelques consultations du site PMMP")
    print("  2. V√©rifier que le champ url_detail est bien rempli")
    print("  3. Afficher les r√©sultats sans sauvegarder en DB")
    print("\nLe test s'arr√™te apr√®s avoir extrait quelques items...")
    print("="*70 + "\n")
    
    # Configuration Scrapy
    settings = get_project_settings()
    
    # D√©sactiver les pipelines DB pour le test
    settings['ITEM_PIPELINES'] = {
        'scraper.pipelines.ValidationPipeline': 100,
        'scraper.pipelines.CleaningPipeline': 200,
        # Pas de DB ni d'archivage pour le test
    }
    
    # Limiter le nombre de pages
    settings['CLOSESPIDER_ITEMCOUNT'] = 10  # Arr√™ter apr√®s 10 items
    settings['CONCURRENT_REQUESTS'] = 1
    settings['DOWNLOAD_DELAY'] = 2
    
    # Cr√©er le processus
    process = CrawlerProcess(settings)
    
    # Cr√©er le collecteur
    collector = ItemCollector()
    
    # Connecter le signal
    from scrapy.crawler import CrawlerRunner
    crawler = process.create_crawler('consultations_spider')
    crawler.signals.connect(collector.item_scraped, signal=signals.item_scraped)
    
    # Lancer le spider
    try:
        process.crawl('consultations_spider', statut='en_cours')
        process.start()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
    
    # Afficher les r√©sultats
    print("\n" + "="*70)
    print("  R√âSULTATS DU TEST")
    print("="*70)
    
    total = stats_collector['total_items']
    with_url = stats_collector['items_with_url']
    without_url = stats_collector['items_without_url']
    
    print(f"\nüìä Statistiques:")
    print(f"  Total d'items extraits: {total}")
    print(f"  Avec URL d√©taill√©e: {with_url} ({with_url/total*100 if total > 0 else 0:.1f}%)")
    print(f"  Sans URL d√©taill√©e: {without_url} ({without_url/total*100 if total > 0 else 0:.1f}%)")
    
    if stats_collector['sample_items']:
        print(f"\nüìã Exemples d'items extraits:")
        for i, item in enumerate(stats_collector['sample_items'], 1):
            status = "‚úì" if item['has_url'] else "‚úó"
            print(f"\n  {i}. {status} {item['ref']}")
            print(f"     Titre: {item['titre']}")
            print(f"     Organisme: {item['organisme']}")
            print(f"     URL: {item['url_detail'] or '‚ùå MANQUANT'}")
    
    # Verdict
    print("\n" + "="*70)
    if total > 0:
        if with_url == total:
            print("‚úÖ EXCELLENT: Toutes les consultations ont une URL d√©taill√©e!")
            print("   Le champ url_detail sera correctement rempli dans la base.")
        elif with_url > total * 0.8:
            print("‚úì BON: La majorit√© des consultations ont une URL d√©taill√©e.")
            print(f"  Mais {without_url} items n'ont pas d'URL. V√©rifiez les logs.")
        else:
            print("‚ö†Ô∏è ATTENTION: Beaucoup d'items sans URL d√©taill√©e!")
            print("   Les s√©lecteurs dans scraper/selectors.py doivent √™tre ajust√©s.")
            print("\nüí° Action recommand√©e:")
            print("   1. Ex√©cuter: python scripts/test_extraction_real.py")
            print("   2. Examiner le fichier HTML g√©n√©r√©")
            print("   3. Ajuster ConsultationsSelectors.DETAIL_LINK")
    else:
        print("‚ùå √âCHEC: Aucun item n'a √©t√© extrait!")
        print("   V√©rifiez la connexion au site et les s√©lecteurs.")
    
    print("="*70 + "\n")
    
    return 0 if with_url > 0 else 1


if __name__ == "__main__":
    sys.exit(run_test())
